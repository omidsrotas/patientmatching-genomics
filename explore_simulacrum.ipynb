{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "#import pubchempy as pcp\n",
    "#from chembl_webresource_client.new_client import new_client\n",
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data lobe\n",
    "subscription_id = 'b88e2867-6b66-4723-88c1-25f2d77b0394'\n",
    "resource_group = 'az-srotasengine-dev-uks-001'\n",
    "workspace_name = 'srotasengine'\n",
    "\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "datastore = Datastore.get(workspace, \"workspaceblobstore\")\n",
    "dataset = Dataset.Tabular.from_delimited_files(path=(datastore, 'UI/2024-11-05_052811_UTC/simulacrum/Data'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'infer_column_types': 'False', 'activity': 'to_path'}\n",
      "{'infer_column_types': 'False', 'activity': 'to_path', 'activityApp': 'FileDataset'}\n",
      "{'infer_column_types': 'False', 'activity': 'download'}\n",
      "{'infer_column_types': 'False', 'activity': 'download', 'activityApp': 'FileDataset'}\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 556: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m first_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Data/sim_sact_regimen.csv\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#csv_files[0]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m local_path \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mdownload(target_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m first_csv_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Check if there are any CSV files and read the first one\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# if csv_files:\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#     first_csv_path = csv_files[0]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     print(\"No CSV files found.\")\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/pandas/io/parsers/readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    571\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    572\u001b[0m     dialect,\n\u001b[1;32m    573\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    583\u001b[0m )\n\u001b[1;32m    584\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/pandas/io/parsers/readers.py:482\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    479\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/pandas/io/parsers/readers.py:811\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1040\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (valid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1038\u001b[0m     )\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:69\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     67\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ensure_dtype_objs(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:542\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:642\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1917\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 556: invalid start byte"
     ]
    }
   ],
   "source": [
    "# Assuming the previous code to list CSV files is already present\n",
    "datastore_path = [(datastore, 'UI/2024-11-05_052811_UTC/simulacrum/')]\n",
    "dataset = Dataset.File.from_files(path=datastore_path)\n",
    "file_paths = dataset.to_path()\n",
    "csv_files = [file for file in file_paths if file.endswith('.csv')]\n",
    "\n",
    "# Read only the first CSV file\n",
    "first_csv_path = '/Data/sim_sact_regimen.csv' #csv_files[0]\n",
    "local_path = dataset.download(target_path='.', overwrite=True)\n",
    "first_csv_df = pd.read_csv(local_path[0])\n",
    "# Check if there are any CSV files and read the first one\n",
    "# if csv_files:\n",
    "#     first_csv_path = csv_files[0]\n",
    "    \n",
    "#     # Use dataset.download to retrieve the file\n",
    "#     local_path = dataset.download(target_path='.', overwrite=True)\n",
    "    \n",
    "#     # Read the first CSV file using pandas\n",
    "#     first_csv_df = pd.read_csv(local_path[0])  # local_path[0] refers to the downloaded first CSV file\n",
    "    \n",
    "#     # Display the first few rows to confirm it's loaded\n",
    "#     print(first_csv_df.head())\n",
    "# else:\n",
    "#     print(\"No CSV files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process the tumour data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577: DtypeWarning: Columns (14,15,16,17,23,24,25,26) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/home/azureuser/cloudfiles/code/Users/Omid.Bazgir/data/'\n",
    "\n",
    "patient_pd = pd.read_csv(data_dir + 'sim_av_patient.csv')\n",
    "tumour_pd = pd.read_csv(data_dir + 'sim_av_tumour.csv')\n",
    "regimen_pd = pd.read_csv(data_dir + 'sim_sact_regimen.csv')\n",
    "outcomes_pd = pd.read_csv(data_dir + 'sim_sact_outcome.csv')\n",
    "cancer_types = pd.read_json(data_dir + 'cancer_types.json')\n",
    "drug_pd = pd.read_csv(data_dir + 'sim_sact_drug_detail.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can you make json file from the below code\n",
    "Ethnicity = {}\n",
    "Ethnicity['White'] = ['0','A','B','C','CA','CH','CP']\n",
    "Ethnicity['Mixed'] = ['D','E','F','G']\n",
    "Ethnicity['SouthAsian'] = ['H','J','K','L']\n",
    "Ethnicity['Black'] = ['M','N','P']\n",
    "Ethnicity['Asian'] = ['R']\n",
    "Ethnicity['Other'] = ['S','Z','X']\n",
    "# save the dictionary as a json file\n",
    "import json\n",
    "with open('ethnicity.json', 'w') as json_file:\n",
    "    json.dump(Ethnicity, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ethnicity data from the json file\n",
    "with open(data_dir + 'ethnicity.json', 'r') as json_file:\n",
    "    Ethnicity = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the mapping from the Ethnicity dictionary to the patient_pd dataframe ETHNICITY column\n",
    "# Create a reverse mapping dictionary\n",
    "reverse_mapping = {code: ethnicity for ethnicity, codes in Ethnicity.items() for code in codes}\n",
    "\n",
    "# Apply the reverse mapping to the ETHNICITY column\n",
    "patient_pd['ETHNICITY'] = patient_pd['ETHNICITY'].map(reverse_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the patient data from the dataset pickle file\n",
    "patient_data = pd.read_pickle(data_dir + 'patient_data.pkl')\n",
    "merged_demographic = patient_data['demographic']\n",
    "# add ethnicity data from patient_pd to the merged_demographic dataframe for each PATIENTID\n",
    "merged_demographic = merged_demographic.merge(patient_pd[['PATIENTID', 'ETHNICITY']], on='PATIENTID', how='left')\n",
    "patient_data['demographic'] = merged_demographic\n",
    "# save the patient_data as a pickle file\n",
    "patient_data_path = data_dir + 'patient_data.pkl'\n",
    "with open(patient_data_path, 'wb') as file:\n",
    "    pickle.dump(patient_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Merge the dataframes on PATIENTID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GENDER</th>\n",
       "      <th>PATIENTID</th>\n",
       "      <th>Cancer Type</th>\n",
       "      <th>AGE</th>\n",
       "      <th>ENCORE_PATIENT_ID</th>\n",
       "      <th>HEIGHT_AT_START_OF_REGIMEN</th>\n",
       "      <th>WEIGHT_AT_START_OF_REGIMEN</th>\n",
       "      <th>ETHNICITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10300637</td>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td>87</td>\n",
       "      <td>10300637</td>\n",
       "      <td>1.76</td>\n",
       "      <td>79.50</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10300837</td>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td>64</td>\n",
       "      <td>10300837</td>\n",
       "      <td>1.67</td>\n",
       "      <td>78.75</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10300848</td>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td>94</td>\n",
       "      <td>10300848</td>\n",
       "      <td>1.67</td>\n",
       "      <td>107.10</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>10301090</td>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td>69</td>\n",
       "      <td>10301090</td>\n",
       "      <td>1.71</td>\n",
       "      <td>90.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>10301209</td>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td>90</td>\n",
       "      <td>10301209</td>\n",
       "      <td>1.62</td>\n",
       "      <td>68.30</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131307</th>\n",
       "      <td>2</td>\n",
       "      <td>160016392</td>\n",
       "      <td>Colorectal Cancer</td>\n",
       "      <td>81</td>\n",
       "      <td>160016392</td>\n",
       "      <td>1.65</td>\n",
       "      <td>51.20</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131308</th>\n",
       "      <td>2</td>\n",
       "      <td>170006397</td>\n",
       "      <td>Colorectal Cancer</td>\n",
       "      <td>69</td>\n",
       "      <td>170006397</td>\n",
       "      <td>1.70</td>\n",
       "      <td>83.00</td>\n",
       "      <td>SouthAsian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131309</th>\n",
       "      <td>2</td>\n",
       "      <td>190004163</td>\n",
       "      <td>Colorectal Cancer</td>\n",
       "      <td>88</td>\n",
       "      <td>190004163</td>\n",
       "      <td>1.61</td>\n",
       "      <td>69.50</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131310</th>\n",
       "      <td>2</td>\n",
       "      <td>200006421</td>\n",
       "      <td>Colorectal Cancer</td>\n",
       "      <td>56</td>\n",
       "      <td>200006421</td>\n",
       "      <td>1.52</td>\n",
       "      <td>70.40</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131311</th>\n",
       "      <td>2</td>\n",
       "      <td>250001342</td>\n",
       "      <td>Colorectal Cancer</td>\n",
       "      <td>83</td>\n",
       "      <td>250001342</td>\n",
       "      <td>1.76</td>\n",
       "      <td>92.00</td>\n",
       "      <td>SouthAsian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131312 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        GENDER  PATIENTID        Cancer Type  AGE  ENCORE_PATIENT_ID  \\\n",
       "0            1   10300637      Breast Cancer   87           10300637   \n",
       "1            2   10300837      Breast Cancer   64           10300837   \n",
       "2            1   10300848      Breast Cancer   94           10300848   \n",
       "3            1   10301090      Breast Cancer   69           10301090   \n",
       "4            1   10301209      Breast Cancer   90           10301209   \n",
       "...        ...        ...                ...  ...                ...   \n",
       "131307       2  160016392  Colorectal Cancer   81          160016392   \n",
       "131308       2  170006397  Colorectal Cancer   69          170006397   \n",
       "131309       2  190004163  Colorectal Cancer   88          190004163   \n",
       "131310       2  200006421  Colorectal Cancer   56          200006421   \n",
       "131311       2  250001342  Colorectal Cancer   83          250001342   \n",
       "\n",
       "        HEIGHT_AT_START_OF_REGIMEN  WEIGHT_AT_START_OF_REGIMEN   ETHNICITY  \n",
       "0                             1.76                       79.50       White  \n",
       "1                             1.67                       78.75       White  \n",
       "2                             1.67                      107.10       White  \n",
       "3                             1.71                       90.00         NaN  \n",
       "4                             1.62                       68.30       White  \n",
       "...                            ...                         ...         ...  \n",
       "131307                        1.65                       51.20       White  \n",
       "131308                        1.70                       83.00  SouthAsian  \n",
       "131309                        1.61                       69.50       White  \n",
       "131310                        1.52                       70.40       White  \n",
       "131311                        1.76                       92.00  SouthAsian  \n",
       "\n",
       "[131312 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_demographic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processing outcome dataset to create weak label for supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strong labels\n",
    "# save the outcome_label_pd as a pickle file\n",
    "label_pickle = {}\n",
    "\n",
    "# create a dictionary of labels based on the each regimen modification type\n",
    "label_dict = {}\n",
    "# dose reduction is an indication of toxicity and safety issues\n",
    "dose_reduction = {'Y': 0, 'N': None} \n",
    "label_dict['REGIMEN_MOD_DOSE_REDUCTION'] = dose_reduction\n",
    "# early stop is an indication of success of the treatment\n",
    "early_stop = {'Y': 1, 'N': None} \n",
    "label_dict['REGIMEN_MOD_STOPPED_EARLY'] = early_stop\n",
    "\n",
    "# time delay is an indication of safety and efficacy\n",
    "time_delay = {'Y': 0, 'N': None}\n",
    "label_dict['REGIMEN_MOD_TIME_DELAY'] = time_delay\n",
    "# regimen outcome summary is an indication of the failure of the treatment according to the data specifications\n",
    "outcome_summary_dict = {'0': None,'1':0, '2':0, '3':0, '4':0, '5': None}\n",
    "label_dict['REGIMEN_OUTCOME_SUMMARY'] = outcome_summary_dict\n",
    "\n",
    "# make strong labels for the outcomes\n",
    "strong_outcome_label_pd = copy.copy(outcomes_pd)\n",
    "for key in label_dict.keys():\n",
    "    strong_outcome_label_pd[key] = strong_outcome_label_pd[key].map(label_dict[key])\n",
    "\n",
    "label_pickle['strong_label'] = strong_outcome_label_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRONG LABELS\n",
      "0.0    33022\n",
      "Name: Label, dtype: int64\n",
      "0.0    192702\n",
      "Name: Label, dtype: int64\n",
      "0.0    262372\n",
      "Name: Label, dtype: int64\n",
      "0.0    262372\n",
      "1.0     39213\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"STRONG LABELS\")\n",
    "strong_outcome_label_pd['Label'] = strong_outcome_label_pd['REGIMEN_OUTCOME_SUMMARY']\n",
    "print(strong_outcome_label_pd['Label'].value_counts())\n",
    "# if the label is NaN, and REGIMEN_MOD_DOSE_REDUCTION is 0, then the label is 0\n",
    "# find where the label is NaN and REGIMEN_MOD_DOSE_REDUCTION is 0\n",
    "index = strong_outcome_label_pd['Label'].isna() & (strong_outcome_label_pd['REGIMEN_MOD_DOSE_REDUCTION'] == 0)\n",
    "strong_outcome_label_pd.loc[index, 'Label'] = 0\n",
    "print(strong_outcome_label_pd['Label'].value_counts())\n",
    "# if the label is NaN, and REGIMEN_MOD_TIME_DELAY is 0, then the label is 0\n",
    "# find where the label is NaN and REGIMEN_MOD_TIME_DELAY is 0\n",
    "index = strong_outcome_label_pd['Label'].isna() & (strong_outcome_label_pd['REGIMEN_MOD_TIME_DELAY'] == 0)\n",
    "strong_outcome_label_pd.loc[index, 'Label'] = 0\n",
    "print(strong_outcome_label_pd['Label'].value_counts())\n",
    "# if the label is NaN, and REGIMEN_MOD_STOPPED_EARLY is 1, then the label is 1\n",
    "# find where the label is NaN and REGIMEN_MOD_STOPPED_EARLY is 1\n",
    "index = strong_outcome_label_pd['Label'].isna() & (strong_outcome_label_pd['REGIMEN_MOD_STOPPED_EARLY'] == 1)\n",
    "strong_outcome_label_pd.loc[index, 'Label'] = 1\n",
    "print(strong_outcome_label_pd['Label'].value_counts())\n",
    "label_pickle['strong_label'] = strong_outcome_label_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make weak labels for the outcomes\n",
    "# dose reduction is an indication of toxicity and safety issues\n",
    "dose_reduction = {'Y': 0, 'N': 1} \n",
    "label_dict['REGIMEN_MOD_DOSE_REDUCTION'] = dose_reduction\n",
    "# early stop is an indication of success of the treatment\n",
    "early_stop = {'Y': 1, 'N': 0} \n",
    "label_dict['REGIMEN_MOD_STOPPED_EARLY'] = early_stop\n",
    "\n",
    "# time delay is an indication of safety and efficacy\n",
    "time_delay = {'Y': 0, 'N': 1}\n",
    "label_dict['REGIMEN_MOD_TIME_DELAY'] = time_delay\n",
    "# regimen outcome summary is an indication of the failure of the treatment according to the data specifications\n",
    "outcome_summary_dict = {'0': 1,'1':0, '2':0, '3':0, '4':0, '5': 1}\n",
    "label_dict['REGIMEN_OUTCOME_SUMMARY'] = outcome_summary_dict\n",
    "\n",
    "# make strong labels for the outcomes\n",
    "weak_outcome_label_pd = copy.copy(outcomes_pd)\n",
    "for key in label_dict.keys():\n",
    "    weak_outcome_label_pd[key] = weak_outcome_label_pd[key].map(label_dict[key])\n",
    "\n",
    "label_pickle['weak_label'] = weak_outcome_label_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEAK LABELS\n",
      "1.0    70508\n",
      "0.0    33022\n",
      "Name: Label, dtype: int64\n",
      "1.0    126539\n",
      "0.0     33022\n",
      "Name: Label, dtype: int64\n",
      "0.0    163540\n",
      "1.0    126539\n",
      "Name: Label, dtype: int64\n",
      "0.0    213230\n",
      "1.0    126539\n",
      "Name: Label, dtype: int64\n",
      "0.0    657596\n",
      "1.0    126539\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"WEAK LABELS\")\n",
    "weak_outcome_label_pd['Label'] = weak_outcome_label_pd['REGIMEN_OUTCOME_SUMMARY']\n",
    "print(weak_outcome_label_pd['Label'].value_counts())\n",
    "# if the label is NaN, and REGIMEN_MOD_STOPPED_EARLY is 1, then the label is 1\n",
    "# find where the label is NaN and REGIMEN_MOD_STOPPED_EARLY is 1\n",
    "index = weak_outcome_label_pd['Label'].isna() & (weak_outcome_label_pd['REGIMEN_MOD_STOPPED_EARLY'] == 1)\n",
    "weak_outcome_label_pd.loc[index, 'Label'] = 1\n",
    "print(weak_outcome_label_pd['Label'].value_counts())\n",
    "# if the label is NaN, and REGIMEN_MOD_DOSE_REDUCTION is 0, then the label is 0\n",
    "# find where the label is NaN and REGIMEN_MOD_DOSE_REDUCTION is 0\n",
    "index = weak_outcome_label_pd['Label'].isna() & (weak_outcome_label_pd['REGIMEN_MOD_DOSE_REDUCTION'] == 0)\n",
    "weak_outcome_label_pd.loc[index, 'Label'] = 0\n",
    "print(weak_outcome_label_pd['Label'].value_counts())\n",
    "# if the label is NaN, and REGIMEN_MOD_TIME_DELAY is 0, then the label is 0\n",
    "# find where the label is NaN and REGIMEN_MOD_TIME_DELAY is 0\n",
    "index = weak_outcome_label_pd['Label'].isna() & (weak_outcome_label_pd['REGIMEN_MOD_TIME_DELAY'] == 0)\n",
    "weak_outcome_label_pd.loc[index, 'Label'] = 0\n",
    "print(weak_outcome_label_pd['Label'].value_counts())\n",
    "weak_outcome_label_pd['Label'].fillna(0, inplace=True)\n",
    "print(weak_outcome_label_pd['Label'].value_counts())\n",
    "label_pickle['weak_label'] = weak_outcome_label_pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the label_pickle as a pickle file\n",
    "with open(data_dir + 'label_pickle.pkl', 'wb') as f:\n",
    "    pickle.dump(label_pickle, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post process the genomics data\n",
    "# read the genomics data as the pickle file\n",
    "with open(data_dir + 'patient_data.pkl', 'rb') as f:\n",
    "    patient_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = patient_data['genomics'].columns.tolist()\n",
    "columns.remove('PATIENTID')\n",
    "columns.remove('TUMOURID')\n",
    "gene_pd_merged = patient_data['genomics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = gene_pd_merged['PATIENTID'].unique()\n",
    "gene_aggr_ls = []\n",
    "for ptn_id in patient_id:\n",
    "    gene_ptn = gene_pd_merged[gene_pd_merged['PATIENTID'] == ptn_id]\n",
    "    # in each column set the value to the maximum of the gene expression\n",
    "    gene_ptn = gene_ptn.agg('max')\n",
    "    gene_ptn = pd.DataFrame(gene_ptn).T\n",
    "    gene_aggr_ls.append(gene_ptn)\n",
    "gene_aggr_ls = pd.concat(gene_aggr_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROS1_EXP</th>\n",
       "      <th>NRAS_EXP</th>\n",
       "      <th>CCND2_EXP</th>\n",
       "      <th>CCND1_EXP</th>\n",
       "      <th>CALR_EXP</th>\n",
       "      <th>IDH1_EXP</th>\n",
       "      <th>EGFR_EXP</th>\n",
       "      <th>CD274 (PD-L1)_EXP</th>\n",
       "      <th>ERBB2 (HER2 / NEU)_EXP</th>\n",
       "      <th>BRAF_EXP</th>\n",
       "      <th>...</th>\n",
       "      <th>MPL_DNA</th>\n",
       "      <th>SDHB_DNA</th>\n",
       "      <th>JAK2_DNA</th>\n",
       "      <th>MALT1_DNA</th>\n",
       "      <th>BCL2_DNA</th>\n",
       "      <th>BCL6_DNA</th>\n",
       "      <th>EZH2_DNA</th>\n",
       "      <th>CIC_DNA</th>\n",
       "      <th>HIST1H3B_DNA</th>\n",
       "      <th>IGH_DNA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22561 rows × 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ROS1_EXP  NRAS_EXP  CCND2_EXP  CCND1_EXP  CALR_EXP  IDH1_EXP  EGFR_EXP  \\\n",
       "0          0        -1         -1         -1        -1        -1        -1   \n",
       "0          0        -1         -1         -1        -1        -1        -1   \n",
       "0          0        -1         -1         -1        -1        -1        -1   \n",
       "0         -1         0         -1         -1        -1        -1        -1   \n",
       "0          0        -1         -1         -1        -1        -1        -1   \n",
       "..       ...       ...        ...        ...       ...       ...       ...   \n",
       "0         -1        -1         -1         -1        -1        -1        -1   \n",
       "0          0        -1         -1         -1        -1        -1        -1   \n",
       "0         -1        -1         -1         -1        -1        -1         0   \n",
       "0          0        -1         -1         -1        -1        -1        -1   \n",
       "0         -1         0         -1         -1        -1        -1        -1   \n",
       "\n",
       "    CD274 (PD-L1)_EXP  ERBB2 (HER2 / NEU)_EXP  BRAF_EXP  ...  MPL_DNA  \\\n",
       "0                  -1                      -1        -1  ...       -1   \n",
       "0                  -1                      -1        -1  ...       -1   \n",
       "0                  -1                      -1        -1  ...       -1   \n",
       "0                  -1                      -1         0  ...       -1   \n",
       "0                  -1                      -1        -1  ...       -1   \n",
       "..                ...                     ...       ...  ...      ...   \n",
       "0                  -1                      -1        -1  ...       -1   \n",
       "0                  -1                      -1        -1  ...       -1   \n",
       "0                   3                      -1        -1  ...       -1   \n",
       "0                  -1                      -1         0  ...       -1   \n",
       "0                  -1                      -1        -1  ...       -1   \n",
       "\n",
       "    SDHB_DNA  JAK2_DNA  MALT1_DNA  BCL2_DNA  BCL6_DNA  EZH2_DNA  CIC_DNA  \\\n",
       "0         -1        -1         -1        -1        -1        -1       -1   \n",
       "0         -1        -1         -1        -1        -1        -1       -1   \n",
       "0         -1        -1         -1        -1        -1        -1       -1   \n",
       "0         -1        -1         -1        -1        -1        -1       -1   \n",
       "0         -1        -1         -1        -1        -1        -1       -1   \n",
       "..       ...       ...        ...       ...       ...       ...      ...   \n",
       "0         -1        -1         -1        -1        -1        -1       -1   \n",
       "0         -1        -1         -1        -1        -1        -1       -1   \n",
       "0         -1        -1         -1        -1        -1        -1       -1   \n",
       "0         -1        -1         -1        -1        -1        -1       -1   \n",
       "0         -1        -1         -1        -1        -1        -1       -1   \n",
       "\n",
       "    HIST1H3B_DNA  IGH_DNA  \n",
       "0             -1       -1  \n",
       "0             -1       -1  \n",
       "0             -1       -1  \n",
       "0             -1       -1  \n",
       "0             -1       -1  \n",
       "..           ...      ...  \n",
       "0             -1       -1  \n",
       "0             -1       -1  \n",
       "0             -1       -1  \n",
       "0             -1       -1  \n",
       "0             -1       -1  \n",
       "\n",
       "[22561 rows x 182 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROS1_EXP</th>\n",
       "      <th>NRAS_EXP</th>\n",
       "      <th>CCND2_EXP</th>\n",
       "      <th>CCND1_EXP</th>\n",
       "      <th>CALR_EXP</th>\n",
       "      <th>IDH1_EXP</th>\n",
       "      <th>EGFR_EXP</th>\n",
       "      <th>CD274 (PD-L1)_EXP</th>\n",
       "      <th>ERBB2 (HER2 / NEU)_EXP</th>\n",
       "      <th>BRAF_EXP</th>\n",
       "      <th>...</th>\n",
       "      <th>MPL_DNA</th>\n",
       "      <th>SDHB_DNA</th>\n",
       "      <th>JAK2_DNA</th>\n",
       "      <th>MALT1_DNA</th>\n",
       "      <th>BCL2_DNA</th>\n",
       "      <th>BCL6_DNA</th>\n",
       "      <th>EZH2_DNA</th>\n",
       "      <th>CIC_DNA</th>\n",
       "      <th>HIST1H3B_DNA</th>\n",
       "      <th>IGH_DNA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROS1_EXP  NRAS_EXP  CCND2_EXP  CCND1_EXP  CALR_EXP  IDH1_EXP  EGFR_EXP  \\\n",
       "0        -1         0         -1         -1        -1        -1        -1   \n",
       "\n",
       "   CD274 (PD-L1)_EXP  ERBB2 (HER2 / NEU)_EXP  BRAF_EXP  ...  MPL_DNA  \\\n",
       "0                 -1                      -1         0  ...       -1   \n",
       "\n",
       "   SDHB_DNA  JAK2_DNA  MALT1_DNA  BCL2_DNA  BCL6_DNA  EZH2_DNA  CIC_DNA  \\\n",
       "0        -1        -1         -1        -1        -1        -1       -1   \n",
       "\n",
       "   HIST1H3B_DNA  IGH_DNA  \n",
       "0            -1       -1  \n",
       "\n",
       "[1 rows x 182 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = []\n",
    "for cl in columns:\n",
    "    uni_cl_val = patient_data['genomics'][cl].unique()\n",
    "    for val in uni_cl_val:\n",
    "        if val not in unique_values:\n",
    "            unique_values.append(val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for unique_values\n",
    "unique_values_dict = {}\n",
    "for i, val in enumerate(unique_values):\n",
    "    unique_values_dict[val] = i\n",
    "unique_values_dict[-1]= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every column, map the unique values to integers\n",
    "for cl in columns:\n",
    "    patient_data['genomics'][cl] = patient_data['genomics'][cl].map(unique_values_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43562     10000151\n",
       "69339     10000151\n",
       "43926     10000183\n",
       "43253     10000518\n",
       "115528    10000699\n",
       "            ...   \n",
       "125870    11999966\n",
       "148992    11999966\n",
       "172365    11999966\n",
       "194184    11999966\n",
       "207301    11999966\n",
       "Name: TUMOURID, Length: 66736, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_data['genomics']['TUMOURID']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
