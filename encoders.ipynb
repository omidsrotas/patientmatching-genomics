{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import yaml\n",
    "import logging\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.data_loader import *\n",
    "from models.encoders import *\n",
    "from models.twotower import *\n",
    "import warnings\n",
    "# set all the warnings off\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/azureuser/cloudfiles/code/Users/Omid.Bazgir/data/'\n",
    "# read the genomics data as the pickle file\n",
    "with open(data_dir + 'patient_data.pkl', 'rb') as f:\n",
    "    patient_data = pd.read_pickle(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the label data and drug data as pickle files\n",
    "with open(data_dir + 'label_pickle.pkl', 'rb') as f:\n",
    "    label_data = pd.read_pickle(f)\n",
    "with open(data_dir + 'drug_data_embedding.pkl', 'rb') as f:\n",
    "    drug_data = pd.read_pickle(f)\n",
    "small_molecule_embeddings = drug_data['small_molecule']\n",
    "large_molecule_embeddings = drug_data['large_molecule']\n",
    "labels_df = label_data['weak_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomics_data = patient_data['genomics']\n",
    "demographic_data = patient_data['demographic']\n",
    "# fillna in the ETHINICITY with the most frequent value\n",
    "demographic_data['ETHNICITY'].fillna('White', inplace=True)\n",
    "# Add ETHNICITY to the genomics data using the patient_id\n",
    "genomics_data = genomics_data.merge(demographic_data[['PATIENTID', 'ETHNICITY']], on='PATIENTID', how='left')\n",
    "\n",
    "cancer_types = {}\n",
    "for i, cancer in enumerate(demographic_data['Cancer Type'].unique().tolist()):\n",
    "    cancer_types[cancer] = i\n",
    "\n",
    "# read ethnicity data from the json file\n",
    "with open(data_dir + 'ethnicity.json', 'r') as json_file:\n",
    "    Ethnicity = json.load(json_file)\n",
    "\n",
    "# read the config (yaml) file\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "config['genome_encoder']['input_dim'] = genomics_data.shape[1] - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_types = {}\n",
    "for i, cancer in enumerate(demographic_data['Cancer Type'].unique().tolist()):\n",
    "    cancer_types[cancer] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ethnicity data from the json file\n",
    "with open(data_dir + 'ethnicity.json', 'r') as json_file:\n",
    "    Ethnicity = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(filename='training.log', level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Define batch size\n",
    "batch_size = config['train_param']['batch_size']\n",
    "\n",
    "# Create DataLoader\n",
    "\n",
    "# create a dataloader\n",
    "patient_ids = genomics_data['PATIENTID'].unique().tolist()\n",
    "labels_df = labels_df[labels_df['ENCORE_PATIENT_ID'].isin(patient_ids)]\n",
    "labels_df = labels_df.drop_duplicates(subset=['ENCORE_PATIENT_ID'], keep='first')\n",
    "patient_ids = labels_df['ENCORE_PATIENT_ID'].unique().tolist()\n",
    "# split the patient_ids into train and test\n",
    "patient_train_ids, patient_test_ids = train_test_split(patient_ids, test_size=0.2, random_state=42)\n",
    "genomics_data_train = genomics_data[genomics_data['PATIENTID'].isin(patient_train_ids)]\n",
    "demographic_data_train = demographic_data[demographic_data['PATIENTID'].isin(patient_train_ids)]\n",
    "labels_df_train = labels_df[labels_df['ENCORE_PATIENT_ID'].isin(patient_train_ids)]\n",
    "\n",
    "\n",
    "genomics_data_test = genomics_data[genomics_data['PATIENTID'].isin(patient_test_ids)]\n",
    "demographic_data_test = demographic_data[demographic_data['PATIENTID'].isin(patient_test_ids)]\n",
    "labels_df_test = labels_df[labels_df['ENCORE_PATIENT_ID'].isin(patient_test_ids)]\n",
    "sampler_train = EthnicityBalancedSampler(genomics_data_train, batch_size)\n",
    "patient_dataset_train = PatientDataset(genomics_data_train, demographic_data_train, \n",
    "                                       labels_df_train,small_molecule_embeddings, large_molecule_embeddings,\n",
    "                                        cancer_types, patient_train_ids, Ethnicity)\n",
    "patient_dataloader_train = DataLoader(patient_dataset_train, batch_sampler= sampler_train)\n",
    "\n",
    "sampler_test = EthnicityBalancedSampler(genomics_data_test, batch_size)\n",
    "patient_dataset_test = PatientDataset(genomics_data_test, demographic_data_test, \n",
    "                                      labels_df_test,small_molecule_embeddings, large_molecule_embeddings,\n",
    "                                      cancer_types, patient_test_ids, Ethnicity)\n",
    "patient_dataloader_test = DataLoader(patient_dataset_test, batch_sampler= sampler_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the two tower neural network model\n",
    "1. define the vae model for the patient data encoder\n",
    "    * read the model weights from the pretrained auto-encoder\n",
    "2. define the large molecule encoder\n",
    "3. define the small molecule encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the model \n",
    "input_dim = config['genome_encoder']['input_dim']  # Example input dimension for genomics data\n",
    "hidden_dim = config['genome_encoder']['hidden_dim']  # Example hidden layer dimension\n",
    "latent_dim = config['genome_encoder']['latent_dim']   # Example latent space dimension\n",
    "# Instantiate the VAE\n",
    "genome_encoder = VAE(input_dim, hidden_dim, latent_dim, batch_size).to(device)\n",
    "# read the weights of the model\n",
    "weight_path = '/home/azureuser/cloudfiles/code/Users/Omid.Bazgir/model_weights/genome_vae_model.pth'\n",
    "genome_encoder.load_state_dict(torch.load(weight_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "largeMolEncod = LargeMoleculeEncoder(config['largemolecule_encoder']['input_dim'],\n",
    "                                    config['largemolecule_encoder']['hidden_dim'],\n",
    "                                    config['largemolecule_encoder']['latent_dim']).to(device)\n",
    "smallMolEncod = SmallMoleculeEncoder(config['smallmolecule_encoder']['input_dim'],\n",
    "                                    config['smallmolecule_encoder']['hidden_dim'],\n",
    "                                    config['smallmolecule_encoder']['latent_dim']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_matching = TwoTowerModel(genome_encoder, largeMolEncod, smallMolEncod).to(device)\n",
    "loss_fn = TwoTowerLoss(regularizer=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m train_epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m patient_dataloader_train:\n\u001b[0;32m----> 9\u001b[0m         patient_treatment_similarity, match_score \u001b[38;5;241m=\u001b[39m \u001b[43mpatient_matching\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         match_score_loss, contrastive_loss \u001b[38;5;241m=\u001b[39m loss_fn(patient_treatment_similarity, match_score, batch)\n\u001b[1;32m     11\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/llm-omid-gpu/code/Users/Omid.Bazgir/models/twotower.py:30\u001b[0m, in \u001b[0;36mTwoTowerModel.forward\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m small_mol_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmall_molecule_encoder(smalmol_tensor)\n\u001b[1;32m     29\u001b[0m treatment_embedding \u001b[38;5;241m=\u001b[39m large_mol_embedding \u001b[38;5;241m+\u001b[39m small_mol_embedding\n\u001b[0;32m---> 30\u001b[0m patient_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenome_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdemographics\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m embedding_similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity(patient_embedding, treatment_embedding)\n\u001b[1;32m     32\u001b[0m match_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(embedding_similarity)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)"
     ]
    }
   ],
   "source": [
    "# define the optimizer\n",
    "optimizer = torch.optim.Adam(patient_matching.parameters(), lr=config['train_param']['learning_rate'])\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in range(config['train_param']['n_epochs']+1):\n",
    "        patient_matching.train()\n",
    "        train_epoch_loss = 0\n",
    "        for batch in patient_dataloader_train:\n",
    "                patient_treatment_similarity, match_score = patient_matching(batch, device)\n",
    "                match_score_loss, contrastive_loss = loss_fn(patient_treatment_similarity, match_score, batch)\n",
    "                optimizer.zero_grad()\n",
    "                total_loss = match_score_loss + contrastive_loss\n",
    "                train_epoch_loss += total_loss.item()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                # break\n",
    "                break\n",
    "        # break\n",
    "        break\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        logging.info(f'Epoch {epoch}, Match Score Loss: {match_score_loss}, Contrastive Loss: {contrastive_loss}')\n",
    "        if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Match Score Loss: {match_score_loss}, Contrastive Loss: {contrastive_loss}')\n",
    "        # evaluate the model\n",
    "        patient_matching.eval()\n",
    "        val_epoch_loss = 0\n",
    "        with torch.no_grad():\n",
    "                for batch in patient_dataloader_test:\n",
    "                        patient_treatment_similarity, match_score = patient_matching(batch, device)\n",
    "                        match_score_loss, contrastive_loss = loss_fn(patient_treatment_similarity, match_score, batch)\n",
    "                        total_loss = match_score_loss + contrastive_loss\n",
    "                        val_epoch_loss += total_loss.item()\n",
    "                val_loss.append(val_epoch_loss)\n",
    "                logging.info(f'Epoch {epoch}, Test Match Score Loss: {match_score_loss}, Test Contrastive Loss: {contrastive_loss}')\n",
    "                if epoch % 10 == 0:\n",
    "                        print(f'Epoch {epoch}, Test Match Score Loss: {match_score_loss}, Test Contrastive Loss: {contrastive_loss}')\n",
    "# plot and save the training loss and validation loss\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.savefig('twotower_loss.png')\n",
    "# save the model weights\n",
    "torch.save(patient_matching.state_dict(), '/home/azureuser/cloudfiles/code/Users/Omid.Bazgir/model_weights/patient_matching.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
